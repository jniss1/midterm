# CS 506 Competition Write-Up

## Overview

For this assignment, we designed some code in the form of a jupyter notebook to 
predict the star ratings (from 1 to 5) of movie reviews. We began by following the standard data science workflow of exploring the data, extracting features, and finally training and testing different models. Ultimately, we found marginal success (around 48% accuracey) by using methods we learned within the class (such as text preprocessing, TF-IDF for feature extraction, and SVM as a classification model) along with some more extensive outside reasearch (such as using SMOTE for dealing with class imbalances, and using a Random Forest model for classification) whilst training on around 15,000 reviews from the Amazon Movie Review dataset. With all preprocessing held the same, we saw that the Random Forrest model performed better than the SVM by around 8 points. Despite all of the effort into implementing these models and cleaning the data, we ended up seeing the most success with the provided KNN code (53%) by using a massive number of neighbors. However, there is the caveate that by massivley increasing the number of neighbors the model was overfitting to the unbalenced dataset and outputted a majority of 5 star review, and since the underlying review distribution was half 5 star reviews, makes this kind of hackey. This is not the case for the other models which were able to make honest predictions for all classes, and though there is still room for more improvement was still a partial success.

## Preprocessing

When working with text in data science, one relatively simple way to improve model performance is to clean the data to allow the model to have a better opertunity at extracting the important features from the data. Although the dataset came with a couple different columns that we could use as features, it became clear pretty quick that the most valuble information where the text columns. This lead to the idea that we need to come up with some way to quantify the text in order to make predictions. Naturally, this led to what we learned when when working on assignment 4 when building the latent semantic search engine and the TFIDF vectorizer. 

The first step to effectivly useing text is to get rid of the unecesarry information before training on it. Here are the steps we decided to implament in our workflow: First, we made all sentences uniformly lower case to avoid the same word being counted twice as seperate "tokens" in the models. Second, we removed all punctuation from the text to lessen any noise and bring more focus on the important words. In theory, one could use the punctuation as some feature to model something like how enthusiastic or negative a text is (same thing for capitalization such as how people use uppercase letter to represent SPEAKING IN A LOUD OR EMPHASIZED VOICE), but we decided to forgo these approaches in favor of using a TFIDF vectorizer (more on that later). Third, we then split the text into individual words/tokens and removed stop words (like "and or "the") which are necesarry for grammar but do not impart meaning. Finally, to cut down on the number of tokens we convert words to their root forms (i.e. "cooking" to "cook"). Most of these techniques, as we did in class, where implamented using the nlk library. 

After simmering down our raw text imputs, we then fed our "Text" column into a TfidfVectorizer as our main method for extracting features. We also recognized that we could get even more data if we also fed the "Summary" column in as well, but opted not to for simplicity reasons. In hind sight, if we had the time, I think it would have been interesting to see how the acuracy would have changed if we used the "Summary" column instead of the "Text"--this is because we can intepret the task of summarization as another filtering step to get rid of unecesary info like the other preprocessing. It would then be a balence between trying to capture the most text data vs trying to capture the most important text data. 

As we learned in class, a TF-IDF, or term frequency-inverse document fequency, score is one way to measure how relevent a word is to some text. For example, say we see the word "fire" in some text; we would first find out how many times a word appears in a text as some metric for how relevent it is. We would then calculate how often that word appear in all of the texts and divide the former by the latter, doing this for all words. This allows us to quantify how relevent a word is to the document in relation to the whole corpus. Going back to our example, if we where looking at a database of movie reviews, instances of the word "fire" in a coloquial sence would bring attention to possibly more positive review, where as if it where in the context of classifying people's tweet's about forrest fires would be pretty meaningless since most tweets would have the word in it.



## Classification Algorithms

After preprocessing the text and generating the features, we could now train a model to make star rating predictions based of the feature vectors produced by the TF-IDF vectorizer. Here, we used two different types of classification models from sklearn, a linear support vector machine and random forrest to predict the ratings:

### SVM

As we learned in lecture, Support Vector Machines are a pretty good model for creating classes of data. By maximizing the margins between the class seperating planes and the datapoints on both classes, the SVM can make solid predictions on what the underlying classes are. The SVM model also comes equipt with a hyperparameter C which is a number that we can use to tune the trade-off in the SVM optimization between the size of the magins and minimizing classification errors. Intuitivley, having a large C places a higher value on correctly classifying all of the training example leading to a smaller margin, but this risks overfitting to the training data. On the flip size, a smaller C encourages a larer margin meaning more misclassifications in order to get avoid overfitting--however this risks making an overly simplistic model that does not capture the structure of the classes and can lead to more errors. To balence the bias-variance trade-off we can tune C using the function GridSearchCV which allows us to perform cross-validation over different values to pick the best one. 


### Random Forrest 

In class, we learned about descision trees as a tool for classifciation. One model that we learned in another class at BU that utilizes decision trees are Random Forrest models, which combine multiple decision trees together to improve the overall prediction. Here is a general overview of how the model works. In statistics, bootstrap sampling is a technique where one creates multiple subsets of data from a dataset by randomly sampling a dataset with replacement. Thus, for a data set of N samples, we create M different piles of data where certain points can be in multiple pile while others are in none at all. We then instantiate M diffeent decision trees, each trainined on its own group of bootstraped samples, meaning each tree will learn a slightly different pattern. Then, during training, the decision trees make splits based on the features that best separate the classes. Thus, when making a prediction, each feature vector is passed to each tree independently, starting at the root and anding at a leaf for each tree, where the leaf will represent a class. Each tree votes for the class that it ends up on and the majority class is selected. In theory, just like the SVM, the RF has its own hyperparameters such as the number of trees and the max depth that we could optimize for. We did not impliment this since building and training our models ended up taking a lot of time. 



## Additional tricks

One of the first major observations one sees when exploring the data is the its shape. Nearly half of the training set is composed of 5 star reviews, with each subsequent star reciveing exponentially less. It is because of this that the KNN hack works well enough to at over fifty percent acuracy. However, this is also the reason why it is difficult to train the data set. Realistically, if we don't want to wait an unreasonable amount of time to train our models we can only use around 15000 data points. Because of this along with the over representation of 5 star reviews, it becomes difficult for the models to learn the patterns of the lower categories for the 1 and 2 star reviews, and ultimately leads to worse predictions. 

In response to this we researched different methods to how others have dealt with the same issue and learned about SMOTE, or the sythetic minority over-sampling technique. What SMOTE does is select a sample from a minority class and create new samples in accordance to the lines of nearest neighbors. In essence, it interpolates between points to create new data rather than a more naive way of over sampeling by simply duplicaitng points. We can control the number of artificial points using a hyper perameter. This made it so that when selecting the datapoints ot train on we would over sample one and two star review in order to give the models a better chance at learning their patterns. 

This property of the data is also what lead us to test a random forest model in addition to an svm. The logic is as follows: unlike SVM which can get thrown off by the unbalenced dataset when building the classes, a RF model is composed of many tree with their own bootstraped data that each come to their own conclusions for the class. As such, the RF should be able to better handle the unbalenced dataset since the algorithm smooths over noise from the averaging and sampling. At least, that is the thought. In practice, the model did end up doing better than the SVM. 